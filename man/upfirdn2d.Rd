% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/upfirdn2d.R
\name{upfirdn2d}
\alias{upfirdn2d}
\title{Pad, upsample, filter, and downsample a batch of 2D images.}
\usage{
upfirdn2d(
  x,
  f,
  up = 1,
  down = 1,
  padding = 0,
  flip_filter = FALSE,
  gain = 1,
  impl = if (cuda_is_available()) "cuda" else "ref"
)
}
\arguments{
\item{x}{Float32/float64/float16 input tensor of the shape
\code{c(batch_size, num_channels, in_height, in_width)}.}

\item{f}{Float32 FIR filter of the shape
\code{c(filter_height, filter_width)} (non-separable),
\code{filter_taps} (separable), or \code{NULL} (identity).}

\item{up}{Integer upsampling factor. Can be a single integer or a vector of integers
\code{c(x, y)} (default: 1).}

\item{down}{Integer downsampling factor. Can be a single int or a vector
\code{c(x, y)} (default: 1).}

\item{padding}{Padding with respect to the upsampled image. Can be a single number
or a vector \code{c(x, y)} or \code{c(x_before, x_after, y_before, y_after)}
(default: 0).}

\item{flip_filter}{\code{FALSE} = convolution, \code{TRUE} = correlation (default: \code{FALSE}).}

\item{gain}{Overall scaling factor for signal magnitude (default: 1).}

\item{impl}{Implementation to use. Can be \code{'ref'} or \code{'cuda'}
(default: \code{'cuda'} if \code{torch::cuda_is_available() == TRUE}, \code{'ref'} otherwise).}
}
\value{
Tensor of the shape \code{c(batch_size, num_channels, out_height, out_width)}.
}
\description{
Performs the following sequence of operations for each channel:
\enumerate{
\item Upsample the image by inserting N-1 zeros after each pixel (\code{up}).
\item Pad the image with the specified number of zeros on each side (\code{padding}).
Negative padding corresponds to cropping the image.
\item Convolve the image with the specified 2D FIR filter (\code{f}), shrinking it
so that the footprint of all output pixels lies within the input image.
\item Downsample the image by keeping every Nth pixel (\code{down}).
This sequence of operations bears close resemblance to scipy.signal.upfirdn().
The fused op is considerably more efficient than performing the same calculation
using standard PyTorch ops. It supports gradients of arbitrary order.
}
}
